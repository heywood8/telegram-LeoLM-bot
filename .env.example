# Telegram Configuration
# TELEGRAM_BOT_TOKEN: Your Telegram Bot API token (from BotFather). Required for the bot to authenticate with Telegram.
# TELEGRAM_WEBHOOK_URL: If using webhooks, the publicly reachable URL Telegram should call for updates (optional when using polling).
# TELEGRAM_WEBHOOK_SECRET: A secret path or token you use to validate incoming webhook requests (recommended when using webhooks).
TELEGRAM_BOT_TOKEN=your_bot_token_here
TELEGRAM_WEBHOOK_URL=https://your-domain.com/webhook
TELEGRAM_WEBHOOK_SECRET=your_webhook_secret_here

# LLM Configuration
# LLM_PROVIDER: Identifier for which LLM provider to use. Set to 'ollama' to use a local Ollama
# installation (preferred) or Ollama Cloud. Other providers (openai, gpt-oss-20b) remain supported.
LLM_PROVIDER=ollama
# LLM_BASE_URL: For Ollama local HTTP API use `http://localhost:11434` (the default Ollama daemon port).
# When running in Docker use `http://host.docker.internal:11434` or the appropriate host mapping. For Ollama Cloud
# set this to the cloud generate endpoint (for example: https://api.ollama.ai/generate) and set LLM_API_KEY.
LLM_BASE_URL=http://localhost:11434
# LLM_MODEL_NAME: Model identifier. For local Ollama use the model name you installed locally (e.g. "mistral","mistral-7b").
# For this repository's example, replace with the model you have available in your Ollama instance.
LLM_MODEL_NAME=Mistral-Small-24B-Instruct-2501-3bit
# LLM_API_KEY: Optional API key for Ollama Cloud or other HTTP LLM providers. Leave blank for local Ollama CLI/daemon.
LLM_API_KEY=
# LLM_TEMPERATURE: Sampling temperature for generation (0.0 - 1.0; lower = more deterministic).
LLM_TEMPERATURE=0.7
# LLM_MAX_TOKENS: Default maximum tokens to generate per response.
LLM_MAX_TOKENS=2048
# LLM_TIMEOUT: Time in seconds to wait for LLM responses before timing out (provider connection timeout).
LLM_TIMEOUT=60
# LLM_REQUEST_TIMEOUT: Time in seconds for individual LLM request timeout (enforced at service layer).
LLM_REQUEST_TIMEOUT=30
# LLM_RETRY_ATTEMPTS: Number of retry attempts for failed LLM requests.
LLM_RETRY_ATTEMPTS=3
# LLM_RETRY_MIN_WAIT: Minimum wait time in seconds between retry attempts (exponential backoff).
LLM_RETRY_MIN_WAIT=2
# LLM_RETRY_MAX_WAIT: Maximum wait time in seconds between retry attempts (exponential backoff).
LLM_RETRY_MAX_WAIT=10
# LLM_CIRCUIT_BREAKER_FAILURES: Number of consecutive failures before circuit breaker opens.
LLM_CIRCUIT_BREAKER_FAILURES=5
# LLM_CIRCUIT_BREAKER_TIMEOUT: Time in seconds before circuit breaker attempts recovery.
LLM_CIRCUIT_BREAKER_TIMEOUT=60

# Database Configuration
# DATABASE_URL: Full SQLAlchemy-compatible connection string for the primary application database.
# Format: postgresql+asyncpg://<user>:<pass>@<host>:<port>/<database>
DATABASE_URL=postgresql+asyncpg://botuser:password@localhost:5432/telegram_bot

# Redis Configuration
# REDIS_URL: Redis connection string used for caching/session storage. Change host/port for your deployment.
REDIS_URL=redis://localhost:6379/0

# MCP (Modular Capabilities / Plugins) Configuration
# NOTE: All built-in MCPs are currently disabled in the codebase (0 registered plugins)
# The MCP framework exists but no plugins are registered in bot/main.py
# To enable MCPs, you'll need to modify bot/main.py to register plugins

# MCP_FILESYSTEM_ENABLED: Enable the filesystem-based MCP plugin for tools that operate on files.
# MCP_FILESYSTEM_ENABLED=false
# MCP_FILESYSTEM_BASE_PATH: Directory inside the container where the MCP filesystem plugin stores files.
# MCP_FILESYSTEM_BASE_PATH=/tmp/bot_workspace

# MCP_DATABASE_ENABLED: Enable the database-backed MCP plugin (set true if you want persistent MCP entries in DB).
# MCP_DATABASE_ENABLED=false
# MCP_DATABASE_URL: Connection string for the optional MCP database (separate DB/schema if desired).
# MCP_DATABASE_URL=postgresql+asyncpg://botuser:password@localhost:5432/mcp_db

# NOTE: Built-in WebSearchMCP has been removed from the codebase
# For web search functionality, integrate an external MCP service
# See docker-compose.yml for external web-search-mcp service example
# MCP_WEBSEARCH_URL: URL of the external web search MCP HTTP bridge (e.g., http://mcp_web_search:8080)
# MCP_WEBSEARCH_URL=http://localhost:8080

# Security Configuration
# ADMIN_USER_IDS: Comma-separated Telegram user IDs that get admin privileges in the bot.
ADMIN_USER_IDS=123456789,987654321
# SECRET_KEY: Application secret used for signing tokens and other secrets. Change in production.
SECRET_KEY=your-secret-key-here-change-in-production
# JWT_ALGORITHM: Algorithm used to sign JWTs (typically HS256).
JWT_ALGORITHM=HS256
# JWT_EXPIRATION_HOURS: Lifetime of issued JWTs in hours.
JWT_EXPIRATION_HOURS=24

# Rate Limiting
# RATE_LIMIT_USER_REQUESTS: Number of allowed requests per user in the user window.
RATE_LIMIT_USER_REQUESTS=20
# RATE_LIMIT_USER_WINDOW: Window in seconds for per-user rate limiting.
RATE_LIMIT_USER_WINDOW=60
# RATE_LIMIT_GLOBAL_REQUESTS: Global request limit across all users in the global window.
RATE_LIMIT_GLOBAL_REQUESTS=100
# RATE_LIMIT_GLOBAL_WINDOW: Window in seconds for global rate limiting.
RATE_LIMIT_GLOBAL_WINDOW=60

# Resource Limits
# MAX_MESSAGE_LENGTH: Maximum allowed Telegram message length to accept and process.
MAX_MESSAGE_LENGTH=4096
# MAX_HISTORY_MESSAGES: How many past messages to keep in session history.
MAX_HISTORY_MESSAGES=50
# MAX_CONTEXT_TOKENS: Maximum total tokens to keep in context passed to the LLM.
MAX_CONTEXT_TOKENS=8000
# MAX_FILE_SIZE: Maximum file upload size in bytes accepted by the bot.
MAX_FILE_SIZE=20971520
# MAX_CONCURRENT_LLM_CALLS: Maximum number of concurrent LLM requests across the bot.
MAX_CONCURRENT_LLM_CALLS=10
# MAX_MCP_EXECUTION_TIME: Max seconds to allow MCP tool execution before timing out.
MAX_MCP_EXECUTION_TIME=30
# TOOL_EXECUTION_TIMEOUT: Time in seconds for individual tool execution timeout (enforced at handler layer).
TOOL_EXECUTION_TIMEOUT=15
# TOOL_RETRY_ATTEMPTS: Number of retry attempts for failed tool executions.
TOOL_RETRY_ATTEMPTS=2

# Logging Configuration
# LOG_LEVEL: One of DEBUG, INFO, WARNING, ERROR. Controls overall logging verbosity.
LOG_LEVEL=INFO
# LOG_FORMAT: 'json' for structured logs or 'text' for human-readable logs.
LOG_FORMAT=json

# Application Configuration
# APP_HOST: Host the application binds to (inside container). Typically 0.0.0.0 in Docker.
APP_HOST=0.0.0.0
# APP_PORT: Port the bot's HTTP server listens on (used for webhook mode or health endpoints).
APP_PORT=8000
# USE_WEBHOOK: Set to true to use Telegram webhooks instead of polling. Requires TELEGRAM_WEBHOOK_URL.
USE_WEBHOOK=false
